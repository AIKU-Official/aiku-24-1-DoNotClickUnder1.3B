# -*- coding: utf-8 -*-
#!pip install bitsandbytes langchain accelerate
"""datagen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SYHe1PhP86r7lnBu6mIuxfMpGThkKFSP
"""

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

import os
import time
import json
import urllib.request
from typing import List, Dict
import random

import pandas as pd
import numpy as np
import torch
import torch.nn as nn

import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from transformers import TextStreamer, GenerationConfig

from langchain.prompts import PromptTemplate
from langchain.output_parsers import PydanticOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field, validator

class datagen:

  def __init__(
      self,
      pretrained_model_name = "kyujinpy/Ko-PlatYi-6B",
      ):

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
        )

    self.model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name,
        torch_dtype='auto',
        quantization_config=quantization_config,
        low_cpu_mem_usage=True,
        ignore_mismatched_sizes=True
        )

    self.tokenizer = AutoTokenizer.from_pretrained(
        pretrained_model_name,
        use_fast=True
        )

  def data_generate(self, query):
    #prompt = self._get_model_input(query)

    max_new_tokens = len("청춘의 에너지를 담은 나이키 운동화, 당신의 스타일을 자유롭게 표현하세요 .") + 8

    inputs = self.tokenizer(query, return_tensors="pt", add_special_tokens=False).to(self.model.device)
    outputs = self.model.generate(
        **inputs,
        use_cache=True,
        max_new_tokens=max_new_tokens,
        )

    output_text = self.tokenizer.decode(outputs[0])
    return output_text

generator = datagen()

def query_gen(example1, example2, product):
  query = f'''
  너는 제품과 내용이 주어지면 한 줄로 마케팅 문구를 작성하는 마케팅 전문가야

  예시:
    상품: {example1["상품"]}
    담고자 하는 가치: {example1["담고자 하는 가치"]}
    카피라이팅: {example1["카피라이팅"]}

    상품: {example2["상품"]}
    담고자 하는 가치: {example2["담고자 하는 가치"]}
    카피라이팅: {example2["카피라이팅"]}

  위의 예시처럼 답변을 작성해 주세요.

    상품: {product[0]}
    담고자 하는 가치: {product[1]}
    카피라이팅:
  '''
  return query

example_file = "/home/aikusrv04/aiku/1.3B/.conda/gpt4_generated.json"
product_file = "/home/aikusrv04/aiku/1.3B/.conda/data_list.json"

def load_json(json_file_path):
    with open(json_file_path, "r", encoding="utf-8") as file:
        data = json.load(file)
    return data

examples = load_json(example_file)
products = load_json(product_file)

print(query_gen(examples[0], examples[1], products[0]))

def extract_out(output):
  output2 = output.split(sep = "카피라이팅:")
  output2 = output2[3].split(sep = "\n")
  return output2[1].strip()

output = generator.data_generate(query_gen(examples[0], examples[1], products[0]))
print(output)
print(extract_out(output))

def random_numbers():
    numbers = list(range(100))
    random.shuffle(numbers)
    return numbers[:2]

def add_result(product, output):
  path = "/home/aikusrv04/aiku/1.3B/.conda/result.json"
  if os.path.exists(path):
      with open(path, 'r', encoding='utf-8') as file:
          data = json.load(file)
  else:
      data = []
  new_data = {
      "상품": product[0],
      "담고자 하는 가치": product[1],
      "카피라이팅": output
  }
  data.append(new_data)
  with open(path, 'w', encoding='utf-8') as file:
      json.dump(data, file, ensure_ascii=False, indent=4)

for _ in range(10000):
  path = "/home/aikusrv04/aiku/1.3B/.conda/index.txt"
  with open(path, 'r', encoding='utf-8') as file:
      i = int(file.readline())
  j = random_numbers()
  output = generator.data_generate(query_gen(examples[j[0]], examples[j[1]], products[i]))
  add_result(products[i], extract_out(output))
  with open(path, 'w', encoding='utf-8') as file:
      file.write(str(i+1))
  print(i+1)